# ======================================================
# üèõÔ∏è TALLER DE HERRAMIENTAS - limpieza_utils.py (ACTUALIZADO)
# ======================================================
import pandas as pd

import re # Necesitaremos el bistur√≠ de texto



def convertir_a_fechas(df: pd.DataFrame, nombre_columna: str) -> pd.DataFrame:
    
    """
    Convertir una Columna a formato fecha, con manejo de Errores.
    """
    
    print(f"\n ----- Columna a Analizar {nombre_columna} ----- \n")
    
    if nombre_columna in df.columns:
        df[nombre_columna] = pd.to_datetime(df[nombre_columna],format='%Y-%m-%d', errors = 'coerce' )
    else:
        print(f"\n ----- ‚ö†Ô∏èAdvertencia: No se encontr√≥ la columna-----")
    return df


def limpiar_num_cols(df: pd.DataFrame, nombre_col ) -> pd.DataFrame: 
    
    """
    Eliminar Caracteres difernetes a N√∫meros
    """
    for col in nombre_col:
        if col in df.columns:
            df[col] = df[col].astype(str) #Asegurar que esta en Formato Texto
            df[col] = df[col].str.replace(r'[^\d.]', '', regex = True)
            df[col] = pd.to_numeric(df[col], errors='coerce')
            print(f'Se convirtio correctamente a numero la columna: {col}')
        else:
            print(f'\n ----- ‚ö†Ô∏èAdvertencia: No se encontr√≥ la columna {col}------')
            
    
    
            

def limpiar_y_convertir_a_numerico(df: pd.DataFrame, nombre_columna: str) -> pd.DataFrame:
    """
    Limpia una columna de texto (quitando no-d√≠gitos) y la convierte a num√©rico.
    """
    print(f"  -> Aplicando herramienta: 'limpiar_y_convertir_a_numerico' en '{nombre_columna}'...")
    if nombre_columna in df.columns:
        # Primero, nos aseguramos de que todo sea texto para poder usar `.str`
        df[nombre_columna] = df[nombre_columna].astype(str)
        
        # El Bistur√≠ Regex: `str.replace(r'[^\d.]', '', regex=True)`
        # `[^\d.]` significa "encuentra cualquier car√°cter que NO SEA (`^`) un d√≠gito (`\d`)
        # o un punto literal (`.`)". Lo reemplaza con nada ('').
        # 'S/. 1,500.50' se convierte en '1500.50'.
        df[nombre_columna] = df[nombre_columna].str.replace(r'[^\d.]', '', regex=True)
        
        # `pd.to_numeric` es el conversor de n√∫meros.
        # `errors='coerce'` convierte cualquier cosa que quede y no sea un n√∫mero
        # (ej. una celda que solo conten√≠a "CANCELADO", ahora vac√≠a) en `NaN`.
        try:
            df[nombre_columna] = pd.to_numeric(df[nombre_columna], errors='coerce')
        except:
            df[nombre_columna] = pd.to_numeric(df[nombre_columna], errors='coerce')

    else:
        print(f"     ‚ö†Ô∏è Advertencia: No se encontr√≥ la columna '{nombre_columna}'. Saltando.")
    return df



def reconstruir_identidades(df: pd.DataFrame) -> pd.DataFrame:
    """
    Usa el DNI para rellenar nombres faltantes y viceversa.
    Usa el m√©todo .loc para evitar advertencias y asegurar la modificaci√≥n.
    """
    print("  -> Aplicando herramienta REFORJADA: 'reconstruir_identidades'...")
    
    # ... (El PASO PREVIO y la CONSTRUCCI√ìN DEL MAPA son iguales) ...
    if 'dni' not in df.columns or 'nombre' not in df.columns:
        print("     ‚ö†Ô∏è Advertencia: Columnas 'dni' y/o 'nombre' no existen.")
        return df


    df['dni'] = df['dni'].astype(str).str.replace(r'\.0$', '', regex=True).replace({'<NA>': None, 'None': None, 'nan': None, '': None})
    df['nombre'] = df['nombre'].replace({'<NA>': None, 'None': None, 'nan': None, '': None})
    df_mapa = df.dropna(subset=['dni', 'nombre']).copy()
    mapa_dni_a_nombre = df_mapa.drop_duplicates(subset=['dni'], keep='last').set_index('dni')['nombre']
    mapa_nombre_a_dni = df_mapa.drop_duplicates(subset=['nombre'], keep='last').set_index('nombre')['dni']
    
    print(f"     -> Mapa de la verdad creado con {len(mapa_dni_a_nombre)} DNI √∫nicos.")

    # --- LA CIRUG√çA DE RECONSTRUCCI√ìN (LA T√âCNICA DEL MAESTRO) ---
    
    # 1. Rellenar nombres usando el DNI
    #    La Condici√≥n: `df['nombre'].isnull()` -> Dame las filas donde el nombre es NULO.
    #    La Columna a Modificar: `'nombre'`
    #    El Valor a Asignar: `df['dni'].map(mapa_dni_a_nombre)` -> La traducci√≥n del DNI a nombre.
    condicion_nombre_nulo = df['nombre'].isnull()
    df.loc[condicion_nombre_nulo, 'nombre'] = df.loc[condicion_nombre_nulo, 'dni'].map(mapa_dni_a_nombre)
    print(f"     -> üìú Se intent√≥ rellenar nombres...")

    # 2. Rellenar DNIs usando el nombre
    condicion_dni_nulo = df['dni'].isnull()
    df.loc[condicion_dni_nulo, 'dni'] = df.loc[condicion_dni_nulo, 'nombre'].map(mapa_nombre_a_dni)
    print(f"     -> üÜî Se intent√≥ rellenar DNIs...")
    
    return df


# ======================================================================
# üèõÔ∏è CAMPO DE PRUEBAS PARA DEPURACI√ìN
# ======================================================================
# La Anatom√≠a: `if __name__ == "__main__":`
# El Prop√≥sito del Artesano: Esta es una construcci√≥n m√°gica en Python.
# El c√≥digo dentro de este `if` SOLO se ejecutar√° cuando corras este archivo
# .py DIRECTAMENTE. Nunca se ejecutar√° cuando lo importes desde tu notebook.
# Es el lugar perfecto y seguro para poner c√≥digo de prueba.

if __name__ == "__main__":
    print("--- üî¨ Iniciando ejecuci√≥n en modo de prueba directa üî¨ ---")
    
    # 1. Creamos un DataFrame de prueba que simule nuestro problema.
    #    Tiene DNIs duplicados y nombres/DNIs faltantes.
    datos_de_prueba = {
        'dni': ['123', '456', '123', None, '789'],
        'nombre': ['Juan Perez', 'Ana Gomez', None, 'Juan Perez', 'Luis Paez']
        
        # A√±ade m√°s columnas si tus funciones las necesitan
    }
    df_prueba = pd.DataFrame(datos_de_prueba)
    
    print("\n--- DataFrame de Prueba Inicial ---")
    print(df_prueba)
    
    # 2. Llamamos a la herramienta que queremos depurar.
    print("\n--- Llamando a la funci√≥n 'reconstruir_identidades'... ---")
    df_resultado = reconstruir_identidades(df_prueba)
    
    # 3. Imprimimos el resultado final.
    print("\n--- DataFrame Resultado ---")
    print(df_resultado)

def convertir_a_categoria(df: pd.DataFrame, lista_columnas: list) -> pd.DataFrame:
    """
    Convierte las columnas especificadas en una lista a tipo 'category'.
    Verifica la existencia de cada columna antes de la conversi√≥n para evitar errores.
    """
    print(f"  -> ‚öôÔ∏è  Aplicando herramienta de optimizaci√≥n: 'convertir_a_categoria'...")
    
    # La Anatom√≠a: `for col in lista_columnas:`
    # El Prop√≥sito del Artesano: Iteramos sobre la lista de columnas que
    # queremos convertir. Este es el enfoque correcto.
    for col in lista_columnas:
        
        # --- EL PASO DE SEGURIDAD CR√çTICO ---
        # La Anatom√≠a: `if col in df.columns:`
        # El Prop√≥sito del Artesano: ANTES de intentar convertir una columna,
        # verificamos si realmente existe en el DataFrame. Esto hace que
        # nuestra funci√≥n sea robusta y a prueba de errores.
        if col in df.columns:
            df[col] = df[col].astype('category')
            print(f"     -> Columna '{col}' convertida a 'category'.")
        else:
            # Si la columna no existe, no rompemos el programa.
            # Simplemente informamos al artesano y continuamos.
            print(f"     -> ‚ö†Ô∏è Advertencia: La columna '{col}' no se encontr√≥ en el DataFrame. Saltando.")
            
    return df



import re

def desglosar_fecha(df: pd.DataFrame, nombre_columna: str, prefijo: str) -> pd.DataFrame:
    """
    Toma una columna de fecha y la desglosa en columnas de a√±o, mes y d√≠a.
    Es eficiente porque solo realiza la conversi√≥n a datetime una vez.
    """
    print(f"  -> üóìÔ∏è  Aplicando herramienta: 'desglosar_fecha' en '{nombre_columna}'...")
    if nombre_columna in df.columns:
        # La conversi√≥n a datetime se hace UNA SOLA VEZ y se guarda.
        fecha_dt = pd.to_datetime(df[nombre_columna], errors='coerce')
        
        df[f'{prefijo}_year'] = fecha_dt.dt.year.astype('Int64')
        df[f'{prefijo}_month'] = fecha_dt.dt.month.astype('Int64')
        df[f'{prefijo}_day'] = fecha_dt.dt.day.astype('Int64')
        
        # Eliminamos la columna original despu√©s de haberla usado.
        df.drop(columns=[nombre_columna], inplace=True)
        print(f"     -> Columna '{nombre_columna}' desglosada y eliminada.")
    else:
        print(f"     ‚ö†Ô∏è Advertencia: No se encontr√≥ la columna '{nombre_columna}'. Saltando.")
    return df


import unicodedata


def _strip_accents(s: pd.Series) -> pd.Series:
    # Normaliza a ASCII, quita acentos y caracteres raros
    return (s.astype(str)
              .fillna("")
              .apply(lambda x: unicodedata.normalize("NFKD", x))
              .str.encode("ascii", "ignore")
              .str.decode("ascii"))

def _prep_text(df: pd.DataFrame, cols_fuente) -> pd.Series:
    if isinstance(cols_fuente, str):
        cols_fuente = [cols_fuente]
    cols_fuente = [c for c in cols_fuente if c in df.columns]
    if not cols_fuente:
        raise ValueError("Ninguna de las columnas fuente existe en el DataFrame.")
    txt = df[cols_fuente].astype(str).agg(" ".join, axis=1)
    txt = _strip_accents(txt).str.upper()
    # Espacios uniformes; dejamos letras/numeros y + para combos (BOTOX+JUVEDERM)
    txt = (txt.str.replace(r"[^A-Z0-9+ ]", " ", regex=True)
              .str.replace(r"\s+", " ", regex=True)
              .str.strip())
    return txt

def extraer_producto_principal(df: pd.DataFrame, cols_fuente, col_salida="producto_principal") -> pd.DataFrame:
    '''
    Escane el texto de toal la columns usando el Catalogo para crear una nueva columna 
    '''
    print(f"  -> üî¨ Regex mejorada: buscando marcas en {cols_fuente}‚Ä¶")

    txt = _prep_text(df, cols_fuente)
    out = pd.Series("OTRO", index=df.index, dtype="object")

    # === DICCIONARIO DE PATRONES (orden = prioridad) ===
    # Usa grupos no capturantes (?: ) y variantes comunes. \b = l√≠mite de palabra.
    patrones = [
        # --- TOXINAS botul√≠nicas ---
        (r"\b(?:BOTOX|VISTABEL)\b",                         "BOTOX"),
        (r"\b(?:DYSPORT|ABOBO(?:TULIN|TOX)A?|\bABOBO\b)\b", "DYSPORT"),
        (r"\b(?:XEOMIN|INCOBO(?:TULIN|TOX)A?)\b",           "XEOMIN"),
        (r"\b(?:J(E|I)UVEAU|PRABO(?:TULIN|TOX)A?)\b",       "JEUVEAU"),
        (r"\b(?:NABOTA)\b",                                 "NABOTA"),
        (r"\b(?:NEURONOX)\b",                               "NEURONOX"),
        (r"\b(?:BOTULAX)\b",                                "BOTULAX"),
        (r"\b(?:REVANESSE TOX|LETYBO)\b",                   "LETYBO"),  # seg√∫n mercado
        # --- BIOESTIMULADORES (CaHA, PLLA, PCL, h√≠bridos) ---
        (r"\b(?:RADIESSE|RADIESE|RADESSE)\b",               "RADIESSE"),
        (r"\b(?:SCULPTRA|PLLA)\b",                          "SCULPTRA"),
        (r"\b(?:ELLANSE|ELLANCE)\b",                        "ELLANSE"),
        (r"\b(?:HARMONYCA|HARMON(Y|I)CA|H ARMONYCA|HA RMONYCA)\b", "HARMONYCA"),  # HA + CaHA (Allergan)
        # --- HA FILLERS (familias y sub-marcas) ---
        (r"\b(?:JUVEDERM|J(U|V)EDERM|VOLUMA|VOLIFT|VOLBELLA|VOLITE|VOLUX)\b", "JUVEDERM"),
        (r"\b(?:RESTYLANE|LYFT|REFYNE|DEFYNE|KYSSE|SKINBOOSTERS? RESTYLANE?)\b", "RESTYLANE"),
        (r"\b(?:BELOTERO|INTENSE|BALANCE|SOFT|VOLUME)\b",   "BELOTERO"),
        (r"\b(?:TEOSYAL|TEOXANE|RHA ?[1-4])\b",             "TEOSYAL"),
        (r"\b(?:STYLAGE)\b",                                 "STYLAGE"),
        (r"\b(?:PRINCESS|SAYPHA|CROMA)\b",                   "CROMA/SAYPHA"),
        (r"\b(?:REVANESSE|VERSA)\b",                         "REVANESSE"),
        (r"\b(?:NEAUVIA)\b",                                 "NEAUVIA"),
        (r"\b(?:YVOIRE)\b",                                  "YVOIRE"),
        (r"\b(?:ALIA?XIN)\b",                                "ALIAXIN"),
        (r"\b(?:ART ?FILLER|ARTFILLER|FILORGA)\b",           "ART FILLER"),
        # --- SKINBOOSTERS / PROFILADO ---
        (r"\b(?:PROFHILO)\b",                                "PROFHILO"),
        (r"\b(?:SUNEKOS)\b",                                 "SUNEKOS"),
        (r"\b(?:NCTF|MESOESTETIC|MESOESTETIC ?NCTF)\b",     "NCTF"),
        # --- ENZIMAS/OTROS ADYUVANTES ---
        (r"\b(?:PB ?SERUM|PBSERUM)\b",                       "PB SERUM"),
        (r"\b(?:TIZO)\b",                                    "TIZO"),
    ]

    # Aplica patrones por prioridad; primera coincidencia gana
    for pat, marca in patrones:
        mask = txt.str.contains(pat, regex=True)
        out = out.mask(mask & (out == "OTRO"), marca)

    df[col_salida] = out
    return df


# --- HERRAMIENTA 1: BISTUR√ç DE UNIDADES ---
def extraer_unidades(df, col_fuente) -> pd.DataFrame:
    """
    Extrae la PRIMERA cantidad num√©rica asociada a una unidad de medida
    conocida ('U', 'Jeringa', 'Caja', etc.) de una columna de texto.
    """
    print(f"  -> Applying Multi-Consumption Extractor to '{col_fuente}'...")
    if col_fuente in df.columns:
                # --- EL HECHIZO MAESTRO DE B√öSQUEDA M√öLTIPLE ---
        # La Anatom√≠a: Es casi id√©ntico al anterior, pero ahora con
        # dos grupos de captura.
        # Grupo 1 `(\d+(?:\.\d+)?)`: Captura la cantidad (entero o decimal).
        # Grupo 2 `(U|UND...|JERINGA...)`: Captura la UNIDAD que se encontr√≥.
        patron_universal = r'\b(\d+(?:\.\d+)?)\s*(?:U|UND|UNIDADES|JERINGA|J|JERINGAS|CAJA|CAJAS)\b'
        
        # La Anatom√≠a: `.str.findall()`
        # El Prop√≥sito del Artesano: Esta es la herramienta m√°gica. En lugar
        # de devolver solo la primera coincidencia, `findall` devuelve una
        # LISTA de TODAS las coincidencias encontradas en el texto.
        # Como nuestro patr√≥n tiene dos grupos de captura, cada elemento
        # de la lista ser√° una tupla de dos valores: (cantidad, unidad).
        df['consumos_detectados'] = df[col_fuente].str.findall(
            patron_universal,
            flags=re.IGNORECASE
        )
        
        print("     -> ‚úÖ Columna 'consumos_detectados' creada con listas de tuplas (cantidad, unidad).")
    return df
    
    
    """
    Extrae unidades (ej. 50U, 64 UND) de una columna de texto.
    
    print(f"  -> Aplicando bistur√≠ de 'unidades' en '{col_fuente}'...")
    if col_fuente in df.columns:
        pat_unidades = r'(?i)\b(\d+(?:[.,]\d{3})*|\d+)\s*(?:U(?:ND|NID|NIDADES)?)\b'
        df['unidades'] = (df[col_fuente].astype(str)
                                      .str.extract(pat_unidades, expand=False)
                                      .str.replace(r'[.,]', '', regex=True)
                                      .astype('Int64'))
    return df
    """

# --- HERRAMIENTA 2: SENSOR DE DEUDA ---
def marcar_deuda(df: pd.DataFrame, col_fuente: str = 'notas') -> pd.DataFrame:
    """
    Crea una columna booleana 'deuda' si la palabra 'DEUDA' aparece.
    """
    print(f"  -> Aplicando sensor de 'deuda' en '{col_fuente}'...")
    if col_fuente in df.columns:
        df['deuda'] = df[col_fuente].astype(str).str.contains(r'(?i)\bDEUDA\b', na=False)
    return df

# --- HERRAMIENTA 3: EXTRACTOR DE MONTO DE DEUDA ---
def extraer_monto_deuda(df: pd.DataFrame, col_fuente: str = 'notas') -> pd.DataFrame:
    """
    Extrae un monto num√©rico asociado a la palabra 'DEUDA'.
    """
    print(f"  -> Aplicando extractor de 'monto_deuda' en '{col_fuente}'...")
    if col_fuente in df.columns:
        notas_str = df[col_fuente].astype(str)
        pat1 = r'(?i)DEUDA[^0-9]*[S$\/]*\s*(?:USD|US\$|S\/|\$)?\s*(\d+(?:[.,]\d{3})*(?:[.,]\d{2})?)'
        pat2 = r'(?i)DEUDA[^0-9]*([\d.,]+)\s*(?:USD|US\$|DOLARES?|SOLES?)?'
        monto1 = notas_str.str.extract(pat1, expand=False)
        monto2 = notas_str.str.extract(pat2, expand=False)
        
        monto_combinado = monto1.fillna(monto2).str.replace(r'[.,](?=\d{3}\b)', '', regex=True)
        df['deuda_monto'] = pd.to_numeric(monto_combinado.str.replace(',', '.', regex=False), errors='coerce')
    return df



def consolidar_marcador_problematico(df: pd.DataFrame) -> pd.DataFrame: 
    '''
    Crea una Unica Columna Booleana `paciente_problematico` a partir de do fuentes:
        - Columna `nombre`: Se idnica en algunos Registro entre Parentesisi los     siguiente "(PP)" Indicando esto
        - En la Misma Columns (pp)
    '''
    print( "\n -> consolidating `problematic patient` markers ....")
    
    nombre_contiene_pp = df['nombre'].astype(str).str.contains(r'\(PP\)', na = False)
    
    # `.notnull()` nos da `True` para cualquier fila donde 'pp' NO sea nulo.

    pp_no_es_nulo = df['pp'].notnull() if 'pp' in df.columns else pd.Series(False, index = df.index)
    
    # `|` (La Barra Vertical) es el operador "O" en Pandas
    # El Proposito: un paciente es problmetico (True/False)
    
    df['paciente_problematico'] = nombre_contiene_pp | pp_no_es_nulo 
    
    #Descartamos la column Original 'PP'  ahora que es redundante. 
    if 'pp' in df.columns:
        df.drop(columns = ['pp'], inplace = True)
        
    print("\n -> Columns 'paciente_problematico' creada y 'pp' eliminada " )
    return df

def limpiar_nombre_problematico(df: pd.DataFrame) -> pd.DataFrame: 
        """
        Eliminar el Marcador '(PP)'  y espacios extra de la columna 'nombre'
        """
        print("\n  ->  Cleaning Problematic patient markers from 'nombre' ...")
        if 'nombre' in df.columns: 
            # Usamos el Mismo patron para reemplazarlo con nada y luego limpiar espacios
            df['nombre'] = (df['nombre'].astype(str).str.replace(r'\s*\([^)]*\)', '' , regex = True).str.strip())
        return df
                    



def marcar_deuda_con_contexto_reforjado(df: pd.DataFrame, col_fuente: str = 'notas') -> pd.DataFrame:
    """
    Crea una columna booleana 'deuda_generada' usando una estrategia de
    dos pasos para evitar el error de 'fixed-width look-behind'.
    """
    print(f"  ->  Applying reinforced contextual debt sensor to '{col_fuente}'...")
    if col_fuente in df.columns:
        notas_str = df[col_fuente].astype(str).str.upper()
        
        # --- PASO 1: Marcar TODAS las posibles deudas (El Guardia Ingenuo) ---
        deuda_potencial = notas_str.str.contains(r'\bDEUDA\b', na=False)
        
        # --- PASO 2: Marcar las EXCEPCIONES (La Lista Negra) ---
        # Buscamos los patrones que anulan una deuda.
        patron_excepcion = r'(?:CANCEL[AO]|PAGO)\s+\bDEUDA\b'
        es_excepcion = notas_str.str.contains(patron_excepcion, na=False)
        
        # --- PASO 3: LA L√ìGICA FINAL ---
        # Una deuda es real si es una deuda potencial Y NO es una excepci√≥n.
        df['deuda_generada'] = deuda_potencial & (~es_excepcion)
        
        print("     -> Columna 'deuda_generada' creada con l√≥gica de contexto reforjada.")
    
    # Renombramos la columna `deuda` si existe de una ejecuci√≥n anterior.
    if 'deuda' in df.columns:
        df.drop(columns=['deuda'], inplace=True, errors='ignore')
        
    return df

# ======================================================
# üèõÔ∏è TALLER DE HERRAMIENTAS - EL "COALESCE" DE PANDAS
# ======================================================
import pandas as pd

def consolidar_informacion_paciente(df: pd.DataFrame, columnas_a_consolidar: list) -> pd.DataFrame:
    """
    Simula la funci√≥n COALESCE de SQL a nivel de grupo.
    Para cada paciente (agrupado por DNI), rellena los valores nulos en las
    columnas especificadas usando la informaci√≥n m√°s reciente de otras visitas.
    """
    print(f"  -> ‚ú® Aplicando COALESCE de Pandas para las columnas: {columnas_a_consolidar}...")
    
    if 'dni' not in df.columns:
        print("     ‚ö†Ô∏è Advertencia: No se encontr√≥ la columna 'dni'. Abortando consolidaci√≥n.")
        return df
    
    # Bucle sobre cada atributo que queremos consolidar (telefono, distrito, etc.)
    for col in columnas_a_consolidar:
        if col in df.columns:
            print(f"     -> Consolidando '{col}'...")
            
            # --- EL HECHIZO M√ÅGICO ---
            # 1. Agrupar por DNI: Crea "expedientes" para cada paciente.
            # 2. Seleccionar la Columna: Enf√≥cate solo en la columna actual (ej. 'telefono').
            # 3. Transform('last'):
            #    - DENTRO de cada expediente, encuentra el √∫ltimo valor no nulo.
            #    - CREA una nueva columna en memoria donde ese valor se "transmite"
            #      a todas las filas de ese paciente.
            valores_propagados = df.groupby('dni')[col].transform('last')
            
            # --- LA CIRUG√çA DE TRASPLANTE ---
            # `fillna()` rellena los `NaN` en la columna original
            # con los valores de nuestra columna reci√©n creada en memoria.
            # Es como decir: "Si el tel√©fono de esta visita es nulo,
            # usa el tel√©fono maestro que encontramos para este paciente".
            df[col] = df[col].fillna(valores_propagados)

    return df

# Funcion para poder Estandarizar el Texto de las Filas a Explorar porfavor

import re
import unicodedata




def estandarizar_texto(df: pd.DataFrame, nombre_columna: str) -> pd.DataFrame: 
    """
    Realiza una limpieza profunda sobre una columna de texto y provee un informe
    de auditor√≠a para asegurar que no se pierda informaci√≥n.
    """
    print(f"\n--- üíß Aplicando Purificaci√≥n Profunda (Auditada) en '{nombre_columna}' ---")
    
    if nombre_columna not in df.columns:
        print(f"     ‚ö†Ô∏è Advertencia: No se encontr√≥ la columna '{nombre_columna}'. Saltando.")
        return df

    # --- Auditor√≠a Inicial ---
    conteo_antes = df[nombre_columna].notna().sum()
    print(f"     -> üìä Estado Inicial: {conteo_antes} valores no nulos.")

    # --- El Proceso de Purificaci√≥n (Sintaxis Moderna) ---
    texto_series = df[nombre_columna].fillna('').astype(str)

    # El Hechizo Moderno para quitar acentos:
    # La Anatom√≠a:
    # 1. `unicodedata.normalize('NFKD', x)`: Descompone '√©' en 'e' + '¬¥'.
    # 2. `.encode('ascii', 'ignore')`: Convierte el resultado a BYTES. Ignora
    #    cualquier cosa que no sea ASCII (el acento '¬¥' es descartado).
    # 3. `.decode('ascii')`: Convierte los BYTES de vuelta a un STRING limpio.
    #    Ahora, el hechizo .decode() se aplica al objeto de BYTES, lo cual es correcto.
    def quitar_acentos(texto: str) -> str:
        return unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('ascii')

    texto_sin_acentos = texto_series.apply(quitar_acentos)
    
    texto_limpio = (texto_sin_acentos
                    .str.lower()
                    .str.strip()
                   )
    
    df[nombre_columna] = texto_limpio.replace('', None)
    
    # --- Auditor√≠a Final ---
    conteo_despues = df[nombre_columna].notna().sum()
    print(f"     -> üìä Estado Final:   {conteo_despues} valores no nulos.")
    # ... (l√≥gica de veredicto) ...
           
    perdida_de_datos = conteo_antes - conteo_despues
    if perdida_de_datos < conteo_antes * 0.90: 
           print(f" -> Peridad de Informacion minima")
    else: 
        print(f"\nPerdida de datos: {perdida_de_datos} mayor a lo espera.\nRequiere Revisar!!")
        
    return df



# --- Importando el cerebro desde nuestro taller de conocimiento ---
# Esto asume que 'catalogo_conocimiento.py' est√° en la misma carpeta 'src'.


# --- HERRAMIENTA 2: EXTRACTOR/INFERIDOR DE SERVICIOS ---
# En `limpieza_utils.py`
import pandas as pd
import re
from src.catalogo import GRIMORIO_DE_MARCAS, GRIMORIO_DE_SERVICIOS, S_TOXINA # Importamos los grimorios
'''
# --- HERRAMIENTA 1: DETECTOR DE MARCAS ---
def extraer_marcas(df: pd.DataFrame, col_fuente: str) -> pd.DataFrame:
    """Escanea el texto y extrae una lista de MARCAS can√≥nicas."""
    print(f"  -> üè∑Ô∏è  Aplicando Detector de Marcas en '{col_fuente}'...")
    
    def encontrar_marcas(texto: str) -> list:
        if pd.isna(texto): return []
        encontradas = set()
        for marca_canon, data in GRIMORIO_DE_MARCAS.items():
            for patron in data['patrones']:
                if re.search(patron, texto, re.IGNORECASE):
                    encontradas.add(marca_canon)
        return list(encontradas)
    
    df['marcas_detectadas'] = df[col_fuente].apply(encontrar_marcas)
    return df

# --- HERRAMIENTA 2: EXTRACTOR/INFERIDOR DE SERVICIOS ---
def extraer_servicios(df: pd.DataFrame, col_fuente: str) -> pd.DataFrame:
    """
    Extrae una lista de SERVICIOS. Infiere el servicio a partir de las
    marcas detectadas y tambi√©n busca t√©rminos de servicios gen√©ricos.
    """
    print(f"  -> ‚öôÔ∏è  Aplicando Extractor/Inferidor de Servicios en '{col_fuente}'...")
    
    def encontrar_servicios(row) -> list:
        texto = row[col_fuente]
        marcas_ya_detectadas = row['marcas_detectadas']
        
        if pd.isna(texto) and not marcas_ya_detectadas: return []
        
        servicios_encontrados = set()
        
        # 1. Inferir desde las marcas (la l√≥gica m√°s importante)
        for marca in marcas_ya_detectadas:
            servicio_asociado = GRIMORIO_DE_MARCAS[marca]['servicio']
            servicios_encontrados.add(servicio_asociado)

        # 2. Buscar servicios gen√©ricos en el texto
        for patron, servicio_canon in GRIMORIO_DE_SERVICIOS.items():
            if re.search(patron, texto, re.IGNORECASE):
                servicios_encontrados.add(servicio_canon)
        
        # 3. Caso especial para 'botox' que es marca y servicio a la vez
        if "BOTOX" in marcas_ya_detectadas:
             servicios_encontrados.add(S_TOXINA)

        return list(servicios_encontrados)

    # `.apply(..., axis=1)` es crucial. Le dice a Pandas que pase la FILA ENTERA
    # a nuestra funci√≥n, para que podamos acceder tanto a 'texto_consulta' como a 'marcas_detectadas'.
    df['servicios_realizados'] = df.apply(encontrar_servicios, axis=1)
    return df
'''

# ======================================================
# üèõÔ∏è TALLER DE HERRAMIENTAS - V4.1 (Con Extracci√≥n Ordenada)
# ======================================================
from src.catalogo import GRIMORIO_DE_MARCAS, GRIMORIO_DE_SERVICIOS

# --- HERRAMIENTA 1: DETECTOR DE MARCAS (ORDENADO) ---
def extraer_marcas_ordenado(df: pd.DataFrame, col_fuente: str) -> pd.DataFrame:
    """Escanea el texto y extrae una lista de MARCAS can√≥nicas EN EL ORDEN EN QUE APARECEN."""
    print(f"  -> üè∑Ô∏è  Aplicando Detector de Marcas Ordenado en '{col_fuente}'...")
    
    def encontrar_marcas(texto: str) -> list:
        if pd.isna(texto): return []
        
        # La Anatom√≠a: `re.finditer()`
        # El Prop√≥sito del Artesano: En lugar de un simple `search`, `finditer`
        # es un hechizo que encuentra TODAS las apariciones de un patr√≥n
        # y nos devuelve su posici√≥n en el texto.
        
        hallazgos = []
        for marca_canon, data in GRIMORIO_DE_MARCAS.items():
            for patron in data['patrones']:
                for match in re.finditer(patron, texto, re.IGNORECASE):
                    # Guardamos la marca y su posici√≥n de inicio.
                    hallazgos.append((match.start(), marca_canon))
        
        # Ordenamos los hallazgos por su posici√≥n de inicio.
        hallazgos.sort()
        
        # Eliminamos duplicados manteniendo el orden.
        marcas_ordenadas_unicas = []
        for _, marca in hallazgos:
            if marca not in marcas_ordenadas_unicas:
                marcas_ordenadas_unicas.append(marca)
                
        return marcas_ordenadas_unicas
    
    df['marcas_detectadas'] = df[col_fuente].apply(encontrar_marcas)
    return df

# En `limpieza_utils.py`
from src.catalogo import GRIMORIO_DE_MARCAS, GRIMORIO_DE_SERVICIOS, LISTA_DE_PRIORIDAD_SERVICIOS

# (La funci√≥n `extraer_marcas` puede seguir siendo la ordenada o la simple, no es tan cr√≠tico aqu√≠)

def extraer_servicios_jerarquico(df: pd.DataFrame, col_fuente: str) -> pd.DataFrame:
    """
    Extrae una lista de SERVICIOS y la devuelve ORDENADA seg√∫n la
    jerarqu√≠a de negocio definida en LISTA_DE_PRIORIDAD_SERVICIOS.
    """
    print(f"  -> üëë Aplicando Extractor Jer√°rquico de Servicios en '{col_fuente}'...")
    
    def encontrar_y_ordenar_servicios(row) -> list:
        texto = row[col_fuente]
        marcas_detectadas = row['marcas_detectadas']
        
        if pd.isna(texto) and not marcas_detectadas: return []
        
        # Usamos un SET para la detecci√≥n inicial, por su eficiencia y manejo de duplicados.
        servicios_encontrados_set = set()
        
        # 1. Inferir desde las marcas
        for marca in marcas_detectadas:
            servicio_asociado = GRIMORIO_DE_MARCAS.get(marca, {}).get('servicio')
            if servicio_asociado:
                servicios_encontrados_set.add(servicio_asociado)

        # 2. Buscar servicios gen√©ricos
        for patron, servicio_canon in GRIMORIO_DE_SERVICIOS.items():
            if re.search(patron, texto, re.IGNORECASE):
                servicios_encontrados_set.add(servicio_canon)
        
        # 3. EL ORDENAMIENTO FINAL (La L√≥gica Maestra)
        # La Anatom√≠a: `sorted(lista, key=LISTA_DE_PRIORIDAD.index)`
        # El Prop√≥sito del Artesano: Este es un hechizo de ordenamiento avanzado.
        # Le decimos a Python: "Ordena `servicios_encontrados_set`, pero no lo
        # hagas alfab√©ticamente. En su lugar, para cada servicio, busca su
        # POSICI√ìN (`.index()`) en mi `LISTA_DE_PRIORIDAD_SERVICIOS` y
        # ordena bas√°ndote en ese n√∫mero de posici√≥n (de menor a mayor)."
        # Es la implementaci√≥n directa de tu visi√≥n.
        servicios_ordenados = sorted(
            list(servicios_encontrados_set),
            key=lambda s: LISTA_DE_PRIORIDAD_SERVICIOS.index(s) if s in LISTA_DE_PRIORIDAD_SERVICIOS else 999
        )
        
        return servicios_ordenados

    df['servicios_realizados'] = df.apply(encontrar_y_ordenar_servicios, axis=1)
    return df


'''
# ======================================================
# üèõÔ∏è TALLER DE HERRAMIENTAS - EL EXTRACTOR DE EVENTOS COMPUESTOS
# ======================================================
import pandas as pd
import re
from src.catalogo import GRIMORIO_DE_MARCAS, LISTA_DE_PRIORIDAD_SERVICIOS

def extraer_eventos_de_consulta(df: pd.DataFrame, col_fuente: str) -> pd.DataFrame:
    """
    Extrae una lista de "objetos" (diccionarios), donde cada objeto representa
    un servicio/producto consumido, con su cantidad y unidad asociadas.
    """
    print(f"  -> ‚öõÔ∏è  Aplicando Extractor de Eventos Compuestos en '{col_fuente}'...")
    
    def encontrar_eventos(texto: str) -> list:
        if pd.isna(texto): return []
        
        # --- 1. Detecci√≥n de Marcas y sus Posiciones ---
        hallazgos_marcas = []
        for marca_canon, data in GRIMORIO_DE_MARCAS.items():
            for patron in data['patrones']:
                for match in re.finditer(patron, texto, re.IGNORECASE):
                    hallazgos_marcas.append({
                        "posicion": match.start(),
                        "marca": marca_canon,
                        "servicio": data['servicio']
                    })
        
        # --- 2. Detecci√≥n de Cantidades y sus Posiciones ---
        hallazgos_cantidades = []
        patron_cant = r'\b(\d+(?:\.\d+)?)\s*(U|UND|UNIDADES|JERINGA|JERINGAS|CAJA|CAJAS)\b'
        for match in re.finditer(patron_cant, texto, re.IGNORECASE):
            hallazgos_cantidades.append({
                "posicion": match.start(),
                "cantidad": float(match.group(1)),
                "unidad": match.group(2).upper()
            })
            
        # --- 3. El Algoritmo de Vinculaci√≥n: "El Vecino M√°s Cercano" ---
        eventos_finales = []
        for marca_info in sorted(hallazgos_marcas, key=lambda x: x['posicion']):
            evento = {
                "marca_detectada": marca_info['marca'],
                "servicio_inferido": marca_info['servicio'],
                "cantidad_detectada": None,
                "unidad_detectada": None
            }
            
            # Buscamos la cantidad m√°s cercana a la marca
            cantidad_mas_cercana = None
            menor_distancia = float('inf')
            
            for cant_info in hallazgos_cantidades:
                distancia = abs(marca_info['posicion'] - cant_info['posicion'])
                if distancia < menor_distancia and distancia < 20: # Umbral de 20 caracteres
                    menor_distancia = distancia
                    cantidad_mas_cercana = cant_info
            
            if cantidad_mas_cercana:
                evento['cantidad_detectada'] = cantidad_mas_cercana['cantidad']
                # Normalizamos la unidad
                unidad = cantidad_mas_cercana['unidad']
                if unidad in ['U', 'UND', 'UNIDADES']:
                    evento['unidad_detectada'] = 'unidades'
                elif unidad in ['J', 'JERINGA', 'JERINGAS']:
                    evento['unidad_detectada'] = 'jeringas'
                else:
                    evento['unidad_detectada'] = 'cajas' # Asumimos por defecto
                
                # Removemos la cantidad usada para que no se asigne a otra marca
                hallazgos_cantidades.remove(cantidad_mas_cercana)

            eventos_finales.append(evento)
            
        return eventos_finales

    df['eventos_consulta'] = df[col_fuente].apply(encontrar_eventos)
    return df
'''

# ... (dentro de limpieza_utils.py) ...

from src.catalogo import MAPA_GENERICOS_POR_SERVICIO

def asignar_marcas_genericas(df: pd.DataFrame) -> pd.DataFrame:
    """
    Inspecciona las filas donde 'marcas_detectadas' est√° vac√≠o pero
    'servicios_detectados' no lo est√°, y asigna una marca gen√©rica
    basada en la l√≥gica de negocio del MAPA_GENERICOS_POR_SERVICIO.
    """
    print("  -> ü§ñ Aplicando asignador de marcas gen√©ricas contextuales...")

    # --- La L√≥gica de Aplicaci√≥n por Fila ---
    # La Anatom√≠a: `.apply(funcion, axis=1)`
    # El Prop√≥sito: `.apply` con `axis=1` es una herramienta poderosa pero lenta.
    # Le dice a Pandas: "Toma cada fila, una por una, y p√°sala a la
    # funci√≥n que te doy". Es perfecta para l√≥gicas condicionales complejas
    # que dependen de m√∫ltiples columnas en la misma fila.
    def logica_por_fila(fila):
        servicios = fila['servicios_realizados']
        marcas = fila['marcas_detectadas']

        # Condici√≥n: Si no hay marcas, PERO s√≠ hay servicios...
        if not marcas and servicios:
            # Iteramos sobre los servicios de esta fila
            for servicio in servicios:
                # Consultamos nuestro mapa de gen√©ricos
                if servicio in MAPA_GENERICOS_POR_SERVICIO:
                    # Si encontramos una regla, a√±adimos la marca gen√©rica y salimos.
                    marcas.append(MAPA_GENERICOS_POR_SERVICIO[servicio])
                    break # Solo asignamos una marca gen√©rica por fila.
        return marcas # Devolvemos la lista de marcas (modificada o no)

    # Aplicamos la l√≥gica a la columna 'marcas_detectadas'
    df['marcas_detectadas'] = df.apply(logica_por_fila, axis=1)
    
    print("     -> Asignaci√≥n completada.")
    return df
